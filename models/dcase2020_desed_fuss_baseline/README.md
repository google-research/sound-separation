# README for the DCASE2020 DESED+FUSS baseline model.

This model is the source separation component of the combined sound event
detection and separation baseline for
<a href="http://dcase.community/challenge2020/task-sound-event-detection-and-separation-in-domestic-environments">
DCASE2020 Challenge Task 4: Sound Event Detection and Separation in Domestic
Environments</a>.

You can use the code in this directory to train and evaluate the baseline model
from scratch on DESED+FUSS data. If you find this code or model to be useful,
please cite [1], and please cite [5], [6] for the
<a href=https://github.com/turpaultn/DESED>DESED data</a>.

For this baseline, audio from DESED and FUSS training and validation data are
combined on-the-fly to create new mixtures with both in-domain (DESED) and
open-domain (FUSS) sources. The mixing lists for this on-the-fly combination can
be generated by running:

```
  ./make_baseline_file_lists.sh
```

The sound separation model is trained to separate
these mixtures into three output signals: DESED background, mixture of DESED
foreground sounds, and mixture of FUSS sounds. This model is trained in the same
way as the
<a href=https://github.com/google-research/sound-separation/tree/master/models/dcase2020_fuss_baseline>
baseline SS model</a>, except without permutation invariance. On the DESED+FUSS
validation set, this model achieves an average of 18.6 dB SI-SNR improvement for
the separated DESED foreground mixture, which is used as input to the SED model.

## Install TensorFlow

Follow the instructions
<a href="https://www.tensorflow.org/install">here</a>.

## Evaluate a model

You can download and evaluate the pretrained baseline model using the following:

```
  ./run_baseline_model_evaluate.sh
```

To evaluate a model you have trained yourself, e.g. with
```run_baseline_model_train.sh```, see ```run_baseline_model_evaluate.sh```
for an example of calling ```evaluate.py```.

## Train a model

A pretrained baseline model is provided, but you can also train a model yourself using the following:

```
  ./run_baseline_model_train.sh
```

Training and validation performance can be visualized during training using the
following:

```
  tensorboard --logdir=<your_model_directory>
```

The default model directory is set to

```
${ROOT_DIR}/dcase2020_desed_fuss/fuss_desed_baseline_dry_2_model/${DATE}
```

where ```ROOT_DIR``` is defined in ```setup.sh```, and ```DATE``` has the
following format:

```
<year>-<month>-<day>_<hour>-<minute>-<second>
```

## References

[1] Scott Wisdom, Hakan Erdogan, Daniel P. W. Ellis, Romain Serizel, Nicolas Turpault, Eduardo Fonseca, Justin Salamon, Prem Seetharaman, John R. Hershey,
"What's All the FUSS About Free Universal Sound Separation Data?", 2020, in preparation.

[2] Ilya Kavalerov, Scott Wisdom, Hakan Erdogan, Brian Patton, Kevin Wilson, Jonathan Le Roux, and John R. Hershey. "Universal Sound Separation." IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pp. 175-179. New Paltz, NY, USA, 2019.

[3] Efthymios Tzinis, Scott Wisdom, John R. Hershey, Aren Jansen, and Daniel P. W. Ellis. "Improving Universal Sound Separation Using Sound Classification." IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2020.

[4] Scott Wisdom, John R. Hershey, Kevin Wilson, Jeremy Thorpe, Michael Chinen, Brian Patton, Rif A. Saurous. "Differentiable Consistency Constraints for Improved Deep Speech Enhancement." IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2019.

[5] Nicolas Turpault, Romain Serizel, Ankit Parag Shah, and Justin Salamon. "Sound event detection in domestic environments with weakly labeled data and soundscape synthesis." Workshop on Detection and Classification of Acoustic Scenes and Events, 2019.

[6] Romain Serizel, Nicolas Turpault, Ankit Shah, and Justin Salamon. "Sound event detection in synthetic domestic environments". IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2020.
